# E-Commerce Infrastructure Evolution: A Learning Laboratory

An experimental journey building the same application across four infrastructure paradigms to understand trade-offs between simplicity, cost, control, and scalability.

**What This Is:** A rapid, hands-on exploration of cloud-native patterns over 5 days  
**What This Isn't:** Production-ready e-commerce (intentionally simplified for learning)  
**Funding:** $550 AWS Lift program educational credits (used ~$20, or 3.6%)

## The Experiment

I built identical functionality four times to answer one question: **How do infrastructure choices affect cost, complexity, and capability?**

Each stage ran for approximately 1 day just long enough to deploy, test, understand, and tear down before moving to the next.

```
Stage 1: Single EC2 Instance        â†’  1 day  â†’  $0.40 in credits
Stage 2: Load-Balanced EC2 + RDS    â†’  1 day  â†’  $2.83 in credits
Stage 3: ECS Fargate                â†’  1 day  â†’  $2.00 in credits
Stage 4: Kubernetes + Istio         â†’  3 days â†’  $11.00 in credits (load testing)
```

**Total Duration:** 5 days of focused experimentation  
**Total Cost:** ~$16-20 from $550 in AWS educational credits  
**Application Stack:** Go API, PostgreSQL, Redis, Stripe payments  
**Total Requests Processed:** 239,000+ across all load tests

## Why This Approach

I used AWS Lift program credits specifically designed for proof-of-concept work. The goal wasn't to run production infrastructureâ€”it was to rapidly iterate through deployment patterns, measure the differences, and tear down quickly.

Critics say this is over-engineering. They're rightâ€”but missing the point. This was deliberate over-engineering to understand *why* it's over-engineering. You can't learn the cost of complexity without building complex things and measuring them.

## Key Findings

### Cost Reality Check

| Stage | Monthly Cost | Daily Cost | Actual Runtime | Real Cost |
|-------|--------------|------------|----------------|-----------|
| Stage 1 | $12 | $0.40 | 1 day | $0.40 |
| Stage 2 | $85 | $2.83 | 1 day | $2.83 |
| Stage 3 | $60 | $2.00 | 1 day | $2.00 |
| Stage 4 | $110 | $3.67 | 3 days | $11.00 |
| **Total** | â€” | â€” | **5 days** | **~$16-20** |

**Insight:** Rapid iteration kept costs negligible. Built, tested, understood, tore down. Repeat.

### Performance Under Load

| Stage | 50 Users | 200 Users | Breaking Point | CPU at Failure |
|-------|----------|-----------|----------------|----------------|
| Stage 1 | 45ms | 265ms | ~150 concurrent | 20% |
| Stage 2 | 40ms | 200ms | ~200 concurrent | 18% |
| Stage 3 | 43ms | 185ms | ~500 concurrent | 10% |
| Stage 4 | 47ms | 210ms | ~700 concurrent | 24% |

**Surprise:** CPU was never the bottleneck. Database connection pool exhaustion killed performance before compute resources were saturated.

### Operational Complexity

| Stage | Deploy Time | Config Complexity | Debugging Difficulty |
|-------|-------------|-------------------|----------------------|
| Stage 1 | 5 min | 3 commands | Easy (SSH + logs) |
| Stage 2 | 5 min | 6 commands | Medium (multiple instances) |
| Stage 3 | 2 min | 2 commands | Easy (CloudWatch) |
| Stage 4 | 2 min | 437 lines YAML | High (K8s + mesh) |

**Surprise:** Fargate had the best developer experienceâ€”faster than EC2, simpler than Kubernetes.

### Cost Per Request at Scale

Based on 500,000 req/month if run continuously:

| Stage | Monthly Cost | Cost per Request |
|-------|--------------|------------------|
| Stage 1 | $12 | $0.000024 |
| Stage 2 | $85 | $0.000170 |
| Stage 3 | $60 | $0.000120 |
| Stage 4 | $110 | $0.000220 |

**But I didn't run continuously.** Each stage ran 1-3 days, measured, then destroyed. Total: $16-20.

## The Architecture Evolution

### Stage 1: Single Server Baseline
```
Internet â†’ EC2 (Nginx + Go + PostgreSQL)
Daily cost: $0.40 | Capacity: ~100 users
```

**Runtime:** 1 day  
**Cost:** $0.40  
**Breaking Point:** 150 concurrent users at 20% CPU

Everything on one t2.small instance. Load testing revealed the bottleneck wasn't CPUâ€”it was database connection pooling.

**The lesson:** A single well-configured server handles more than you'd think. 100 concurrent users on $0.40/day is respectable.

[ğŸ“ Stage 1 Infrastructure â†’](./infrastructure/stage-1/)

### Stage 2: Horizontal Scaling
```
Internet â†’ ALB â†’ [EC2 #1, EC2 #2] â†’ RDS PostgreSQL
                                   â†’ ElastiCache Redis
Daily cost: $2.83 | Capacity: ~200 users
```

**Runtime:** 1 day  
**Cost:** $2.83  
**Breaking Point:** 200 concurrent users at 18% CPU

Added load balancer, second instance, external database. Cost increased 7x, capacity increased 33%.

**The lesson:** Horizontal scaling without understanding bottlenecks wastes money. I was scaling compute when I needed to scale database connections.

[ğŸ“ Stage 2 Infrastructure â†’](./infrastructure/stage-2/)

### Stage 3: Managed Containers
```
Internet â†’ ALB â†’ [Fargate Task 1, Fargate Task 2] â†’ RDS
                                                   â†’ Redis
Daily cost: $2.00 | Capacity: ~500 users
```

**Runtime:** 1 day  
**Cost:** $2.00  
**Breaking Point:** 500 concurrent users at 10% CPU

Containerized with Docker, deployed to ECS Fargate. Better performance, lower cost than Stage 2.

**The lesson:** Managed services aren't always more expensive. Fargate was cheaper and easier to operate than multi-instance EC2.

[ğŸ“ Stage 3 Infrastructure â†’](./infrastructure/stage-3-ecs/)

### Stage 4: Kubernetes + Service Mesh
```
Internet â†’ NodePort â†’ [K8s Pod 1, K8s Pod 2] â†’ RDS
                     â†“ (Istio sidecar)        â†’ Redis
              Meshery/Kiali/Grafana
Daily cost: $3.67 | Capacity: ~700 users
```

**Runtime:** 3 days (extended for comprehensive load testing)  
**Cost:** $11.00  
**Breaking Point:** 700 concurrent users at 24% CPU

Self-managed k3s cluster with Istio service mesh. Most complex, most capable, most expensive. Istio added 50-80ms latency per request for observability benefits.

**The lesson:** Kubernetes gives you power at the cost of complexity. For a single application, the trade-off doesn't make sense. For 20 microservices with complex routing needs, it does.

[ğŸ“ Stage 4 Infrastructure â†’](./infrastructure/stage-4-k8s/)

## What Didn't Work

### Failed Attempt: AWS Lambda + API Gateway

**Duration:** 8 hours  
**Outcome:** Abandoned  
**Cost:** $0

SAM CLI tooling for Go Lambda functions was unreliable. Build process repeatedly created empty deployment packages. After 8 hours of debugging tooling instead of learning serverless, I pivoted to containers.

**What I learned:** Knowing when to cut your losses is a skill. Tooling maturity matters.

## Real Bottlenecks Discovered

Through 239,000+ requests across all stages:

**1. Database Connection Pool Exhaustion**
- RDS db.t3.micro: ~87 max connections
- Application pool: 25 connections per instance initially
- This limited scale more than CPU or memory

**2. Connection Pool Mode**
- Switching from session to transaction pooling improved throughput 3x
- Cost: $0, just a config change

**3. Missing Indexes**
- Added indexes on `users.email` and `products.category`
- Query time: 150ms â†’ 12ms
- Cost: $0, just SQL commands

**4. Health Check Frequency**
- Reduced ALB health checks from every 5s to every 30s
- Freed up 20% of connection pool
- Cost: $0, just a config change

**Key Insight:** None of these required new infrastructure. Application and database tuning beat infrastructure scaling.

## The Honest Conclusion

**If I were building this for real, I'd choose Stage 3 (ECS Fargate):**

- Lower cost than multi-instance EC2
- No server management
- Fast deployments (2 minutes)
- Automatic scaling
- Good AWS integration

**I wouldn't choose Stage 4 (Kubernetes) unless:**
- Running multiple microservices (5+)
- Need multi-cloud portability
- Have dedicated ops team
- Advanced traffic management requirements

The K8s setup cost more and delivered less for a single application. But the learning was invaluable for understanding when and why to use it.

## Project Timeline

**Day 1:** Stage 1 - Single EC2 baseline  
**Day 2:** Stage 2 - Horizontal scaling with RDS  
**Day 3:** Stage 3 - Containerization with Fargate  
**Days 4-6:** Stage 4 - Kubernetes + Istio with comprehensive load testing  

**8 hours:** Failed Lambda attempt (learned when to pivot)  

**Total:** ~40 hours over 5 days of focused work

## Lessons Learned

### On Infrastructure

1. **Rapid iteration teaches more than prolonged operation** â€” Build, test, understand, tear down
2. **Managed services are underrated** â€” Fargate beat both EC2 and K8s on cost and ops burden
3. **CPU is rarely the bottleneck** â€” Connection pools, database queries, and network I/O matter more
4. **Complexity has overhead** â€” Istio added 50-80ms latency for observability benefits

### On Learning

1. **Educational credits enable experimentation** â€” $550 budget, used 3.6%, learned everything
2. **Failure is educational** â€” Lambda attempt taught as much as successful deploys
3. **Measure, don't assume** â€” Load testing revealed database as bottleneck, not compute
4. **Build to understand trade-offs** â€” Can't learn when NOT to use K8s without building it

### On Cost

1. **Daily costs enable cheap experimentation** â€” $16-20 total for comprehensive learning
2. **Teardown discipline matters** â€” Destroy after understanding, don't let resources drift
3. **Credits are for learning** â€” AWS Lift program explicitly supports this type of PoC work

## Repository Structure

```
ioc-labs-ecommerce/
â”œâ”€â”€ cmd/api/              # Application entry point
â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ handlers/         # HTTP request handlers
â”‚   â”œâ”€â”€ services/         # Business logic
â”‚   â”œâ”€â”€ repository/       # Database access layer
â”‚   â””â”€â”€ middleware/       # Auth, rate limiting
â”œâ”€â”€ pkg/
â”‚   â”œâ”€â”€ validator/        # Input validation
â”‚   â”œâ”€â”€ errors/           # Error handling
â”‚   â””â”€â”€ database/         # DB connection
â”œâ”€â”€ frontend/             # Vanilla HTML/CSS/JS
â”œâ”€â”€ migrations/           # SQL schema migrations
â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ stage-1/          # Single EC2 setup
â”‚   â”œâ”€â”€ stage-2/          # ALB + multi-instance
â”‚   â”œâ”€â”€ stage-3-ecs/      # Fargate deployment
â”‚   â””â”€â”€ stage-4-k8s/      # Kubernetes manifests
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ load-test-results/
â”‚   â””â”€â”€ stage-summaries/
â””â”€â”€ scripts/              # Deployment automation
```

## What's Missing (Intentionally)

This is a learning project, so I skipped:

- **Password hashing** â€” Using plain text (DO NOT DO THIS IN PRODUCTION)
- **HTTPS/TLS** â€” HTTP only for simplicity
- **CI/CD pipeline** â€” Manual deployments to understand the process
- **Comprehensive tests** â€” Focused on infrastructure, not application logic
- **Advanced security** â€” Basic security groups, no WAF
- **Disaster recovery** â€” No backup strategy

## Addressing Critics

**"This is over-engineering"** â€” Yes, deliberately. You can't learn the cost of complexity without building complex things and measuring them. The conclusion is that simpler was better for this use case.

**"You're wasting money"** â€” Used $16-20 of $550 in educational credits (3.6%). That's efficient use of learning resources.

**"You don't know what you're doing"** â€” The documentation explicitly concludes Stage 4 was overkill. That's judgment, not ignorance.

**"This isn't production-ready"** â€” Correct. It's explicitly labeled as a learning laboratory. The goal was understanding trade-offs, not building production systems.

The project isn't claiming Kubernetes is better. It's proving it's not better for single applications. That required building it to verify.


## License

MIT License - feel free to learn from this, fork it, or use it as a starting point for your own experiments.

## Acknowledgments

This was funded by AWS Lift program educational credits, which provide resources for proof-of-concept development. The rapid iteration approach (1 day per stage) kept costs negligible while maximizing learning.

If you're learning infrastructure, I hope this repository shows that experimentation and honest reflection are more valuable than getting everything right the first time. Sometimes the best way to understand why NOT to use something is to build it and measure it.

---

**Current Status:** All stages torn down after testing  
**Total Cost:** $16-20 from $550 in AWS educational credits (3.6% utilization)  
**Most Valuable Learning:** Complexity for its own sake is expensive. Choose the simplest solution that meets your needs. And sometimes, you need to build the complex thing to understand why the simple thing was better.
